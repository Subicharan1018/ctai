{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d41a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Clean ✓\n",
      "Train full: (14036, 24), Train classification: (14036, 24), Train regression: (14001, 24)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 1 — Load & Clean\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_PATH = r'C:\\Users\\vikym\\Desktop\\ctd\\train.csv'\n",
    "TEST_PATH  = r'C:\\Users\\vikym\\Desktop\\ctd\\test.csv'\n",
    "\n",
    "print(\"Loading...\")\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Parse dates\n",
    "date_columns = ['CONSTRUCTION_START_DATE', 'SUBSTANTIAL_COMPLETION_DATE', 'invoiceDate']\n",
    "for df in (train, test):\n",
    "    for c in date_columns:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "\n",
    "# Drop high-missing example cols if present\n",
    "for c in ['NUMROOMS','NUMBEDS']:\n",
    "    if c in train and train[c].isnull().mean() > 0.5:\n",
    "        train.drop(columns=c, inplace=True, errors='ignore')\n",
    "        test.drop(columns=c, inplace=True, errors='ignore')\n",
    "\n",
    "# Cleaning helpers\n",
    "def clean_numeric_column(s, clip_negative=True, replace_zero_epsilon=False, winsorize=False):\n",
    "    if s.dtype == 'object':\n",
    "        s = (s.str.replace(',', '')\n",
    "               .str.replace('$','')\n",
    "               .str.replace(' ','')\n",
    "               .str.replace(r'[^\\d\\.\\-]','', regex=True))\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "    if clip_negative and s.name == 'QtyShipped':\n",
    "        s = s.clip(lower=0)\n",
    "    if replace_zero_epsilon and s.name in ['UnitPrice','ExtendedPrice']:\n",
    "        s = s.where(s > 0, 0.01)\n",
    "    if winsorize:\n",
    "        lo, hi = s.quantile(0.01), s.quantile(0.99)\n",
    "        s = s.clip(lo, hi)\n",
    "    return s\n",
    "\n",
    "def clean_text_column(s):\n",
    "    if s.dtype == 'object':\n",
    "        s = (s.str.strip()\n",
    "               .str.replace('\\n',' ')\n",
    "               .str.replace('\\r',' ')\n",
    "               .str.lower()\n",
    "               .str.replace(r'[^\\w\\s]',' ', regex=True)\n",
    "               .str.replace(r'\\s+',' ', regex=True))\n",
    "    return s\n",
    "\n",
    "# Clean numeric & text\n",
    "for c in ['ExtendedQuantity','QtyShipped','UnitPrice','ExtendedPrice','invoiceTotal']:\n",
    "    if c in train: train[c] = clean_numeric_column(train[c], clip_negative=(c=='QtyShipped'),\n",
    "                                                   replace_zero_epsilon=(c in ['UnitPrice','ExtendedPrice']),\n",
    "                                                   winsorize=True)\n",
    "    if c in test:  test[c]  = clean_numeric_column(test[c],  clip_negative=(c=='QtyShipped'),\n",
    "                                                   replace_zero_epsilon=(c in ['UnitPrice','ExtendedPrice']),\n",
    "                                                   winsorize=True)\n",
    "for c in ['ItemDescription','PROJECT_CITY','STATE','PROJECT_COUNTRY','CORE_MARKET','PROJECT_TYPE']:\n",
    "    if c in train: train[c] = clean_text_column(train[c])\n",
    "    if c in test:  test[c]  = clean_text_column(test[c])\n",
    "\n",
    "# Don't convert MasterItemNo to numeric - keep as string\n",
    "# Targets & splits\n",
    "train_c = train.copy()  # Use full training set\n",
    "train_r = train.dropna(subset=['QtyShipped']).copy()  # Only rows with QtyShipped for regression\n",
    "\n",
    "# Save cleaned\n",
    "train.to_csv('clean_train_full.csv', index=False)\n",
    "test.to_csv('clean_test_full.csv', index=False)\n",
    "train_c.to_csv('clean_train_c.csv', index=False)\n",
    "train_r.to_csv('clean_train_r.csv', index=False)\n",
    "\n",
    "print(\"Clean ✓\")\n",
    "print(f\"Train full: {train.shape}, Train classification: {train_c.shape}, Train regression: {train_r.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9b7708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing...\n",
      "Saved tfidf_vectorizer.pkl\n",
      "Saved numeric_imputer.pkl\n",
      "Date features shape: (14036, 6)\n",
      "Saved date_imputer.pkl and date_feature_names.pkl\n",
      "Saved categorical_mapping.pkl\n",
      "Features ✓  (X_class_train (14036, 7070), X_reg_train (14001, 7070))\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 2 — Feature Engineering (Full Dataset) - MODIFIED TO SAVE PREPROCESSING\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Featurizing...\")\n",
    "\n",
    "train_c = pd.read_csv('clean_train_full.csv')\n",
    "train_r = pd.read_csv('clean_train_r.csv')\n",
    "test    = pd.read_csv('clean_test_full.csv')\n",
    "\n",
    "# 1) TF-IDF\n",
    "all_desc = pd.concat([train_c['ItemDescription'], test['ItemDescription']]).fillna('missing')\n",
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2, stop_words='english', dtype=np.float32)\n",
    "tfidf.fit(all_desc)\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "print(\"Saved tfidf_vectorizer.pkl\")\n",
    "\n",
    "X_text_train   = tfidf.transform(train_c['ItemDescription'].fillna('missing'))\n",
    "X_text_test    = tfidf.transform(test['ItemDescription'].fillna('missing'))\n",
    "X_text_train_r = tfidf.transform(train_r['ItemDescription'].fillna('missing'))\n",
    "\n",
    "# 2) Numeric (log1p + impute)\n",
    "num_feats = ['ExtendedQuantity','UnitPrice','ExtendedPrice','invoiceTotal']\n",
    "for df in (train_c, train_r, test):\n",
    "    for c in num_feats:\n",
    "        if c in df:\n",
    "            df[c] = df[c].where(df[c] > 0, 0.01)\n",
    "            df[c] = np.log1p(df[c])\n",
    "\n",
    "num_imp = SimpleImputer(strategy='median')\n",
    "X_num_train   = num_imp.fit_transform(train_c[num_feats])\n",
    "X_num_test    = num_imp.transform(test[num_feats])\n",
    "X_num_train_r = num_imp.transform(train_r[num_feats])\n",
    "\n",
    "# Save numeric imputer\n",
    "joblib.dump(num_imp, 'numeric_imputer.pkl')\n",
    "print(\"Saved numeric_imputer.pkl\")\n",
    "\n",
    "# 3) Dates → engineered & imputed - FIXED VERSION\n",
    "date_cols = ['CONSTRUCTION_START_DATE','SUBSTANTIAL_COMPLETION_DATE','invoiceDate']\n",
    "for df in (train_c, train_r, test):\n",
    "    for c in date_cols:\n",
    "        if c in df: df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "\n",
    "def date_feats(df):\n",
    "    out = {}\n",
    "    # Construction duration\n",
    "    if 'CONSTRUCTION_START_DATE' in df and 'SUBSTANTIAL_COMPLETION_DATE' in df:\n",
    "        duration = (df['SUBSTANTIAL_COMPLETION_DATE'] - df['CONSTRUCTION_START_DATE']).dt.days\n",
    "        out['construction_duration_days'] = duration\n",
    "    \n",
    "    # Invoice date features\n",
    "    if 'invoiceDate' in df:\n",
    "        out['invoice_year'] = df['invoiceDate'].dt.year\n",
    "        out['invoice_month'] = df['invoiceDate'].dt.month\n",
    "        out['invoice_day'] = df['invoiceDate'].dt.day\n",
    "        out['invoice_dayofweek'] = df['invoiceDate'].dt.dayofweek\n",
    "        out['invoice_quarter'] = df['invoiceDate'].dt.quarter\n",
    "    \n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Create date features\n",
    "date_train = date_feats(train_c)\n",
    "date_test = date_feats(test)\n",
    "date_train_r = date_feats(train_r)\n",
    "\n",
    "print(f\"Date features shape: {date_train.shape}\")\n",
    "\n",
    "# Impute date features\n",
    "date_imp = SimpleImputer(strategy='median')\n",
    "X_date_train = date_imp.fit_transform(date_train)\n",
    "X_date_test = date_imp.transform(date_test)\n",
    "X_date_train_r = date_imp.transform(date_train_r)\n",
    "\n",
    "# Save date imputer and feature names\n",
    "joblib.dump(date_imp, 'date_imputer.pkl')\n",
    "joblib.dump(list(date_train.columns), 'date_feature_names.pkl')  # Save feature names\n",
    "print(\"Saved date_imputer.pkl and date_feature_names.pkl\")\n",
    "\n",
    "# 4) Categorical (top-k one-hot)\n",
    "cat_cols = ['PROJECT_CITY','STATE','PROJECT_COUNTRY','CORE_MARKET','PROJECT_TYPE','UOM']\n",
    "\n",
    "def one_hot_topk(df, topk=20):\n",
    "    mats, maps = [], {}\n",
    "    for c in cat_cols:\n",
    "        if c in df:\n",
    "            top = df[c].value_counts().head(topk).index.tolist()\n",
    "            maps[c] = top\n",
    "            enc = df[c].fillna('missing').apply(lambda x: x if x in top else 'other')\n",
    "            cols = [ (enc == t).astype(np.int8).to_numpy() for t in (top + ['other']) ]\n",
    "            mats.append(np.column_stack(cols))\n",
    "    return (np.column_stack(mats) if mats else np.empty((len(df),0))), maps\n",
    "\n",
    "X_cat_train, cat_map = one_hot_topk(train_c)\n",
    "\n",
    "# Save categorical mapping\n",
    "joblib.dump(cat_map, 'categorical_mapping.pkl')\n",
    "print(\"Saved categorical_mapping.pkl\")\n",
    "\n",
    "def apply_one_hot(df, cat_map):\n",
    "    mats = []\n",
    "    for c in cat_cols:\n",
    "        if c in df:\n",
    "            top = cat_map.get(c, [])\n",
    "            enc = df[c].fillna('missing').apply(lambda x: x if x in top else 'other')\n",
    "            mats.append(np.column_stack([(enc == t).astype(np.int8).to_numpy() for t in (top + ['other'])]))\n",
    "    return np.column_stack(mats) if mats else np.empty((len(df),0))\n",
    "\n",
    "X_cat_test    = apply_one_hot(test, cat_map)\n",
    "X_cat_train_r = apply_one_hot(train_r, cat_map)\n",
    "\n",
    "# 5) Stack (sparse)\n",
    "X_class_train = sparse.hstack([X_text_train,   sparse.csr_matrix(X_num_train),   sparse.csr_matrix(X_date_train),   sparse.csr_matrix(X_cat_train)]).tocsr()\n",
    "X_class_test  = sparse.hstack([X_text_test,    sparse.csr_matrix(X_num_test),    sparse.csr_matrix(X_date_test),    sparse.csr_matrix(X_cat_test)]).tocsr()\n",
    "X_reg_train   = sparse.hstack([X_text_train_r, sparse.csr_matrix(X_num_train_r), sparse.csr_matrix(X_date_train_r), sparse.csr_matrix(X_cat_train_r)]).tocsr()\n",
    "X_reg_test    = X_class_test.copy()\n",
    "\n",
    "y_class = train_c['MasterItemNo'].to_numpy()\n",
    "y_reg   = train_r['QtyShipped'].to_numpy()\n",
    "\n",
    "# Save with full suffix\n",
    "sparse.save_npz('X_class_train_full.npz', X_class_train)\n",
    "sparse.save_npz('X_class_test_full.npz',  X_class_test)\n",
    "sparse.save_npz('X_reg_train_full.npz',   X_reg_train)\n",
    "sparse.save_npz('X_reg_test_full.npz',    X_reg_test)\n",
    "np.save('y_class_full.npy', y_class)\n",
    "np.save('y_reg_full.npy',   y_reg)\n",
    "\n",
    "print(f\"Features ✓  (X_class_train {X_class_train.shape}, X_reg_train {X_reg_train.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Cell 2 — Feature Engineering (Full Dataset)\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Featurizing...\")\n",
    "\n",
    "train_c = pd.read_csv('clean_train_full.csv')\n",
    "train_r = pd.read_csv('clean_train_r.csv')\n",
    "test    = pd.read_csv('clean_test_full.csv')\n",
    "\n",
    "# 1) TF-IDF\n",
    "all_desc = pd.concat([train_c['ItemDescription'], test['ItemDescription']]).fillna('missing')\n",
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2, stop_words='english', dtype=np.float32)\n",
    "tfidf.fit(all_desc)\n",
    "\n",
    "X_text_train   = tfidf.transform(train_c['ItemDescription'].fillna('missing'))\n",
    "X_text_test    = tfidf.transform(test['ItemDescription'].fillna('missing'))\n",
    "X_text_train_r = tfidf.transform(train_r['ItemDescription'].fillna('missing'))\n",
    "\n",
    "# 2) Numeric (log1p + impute)\n",
    "num_feats = ['ExtendedQuantity','UnitPrice','ExtendedPrice','invoiceTotal']\n",
    "for df in (train_c, train_r, test):\n",
    "    for c in num_feats:\n",
    "        if c in df:\n",
    "            df[c] = df[c].where(df[c] > 0, 0.01)\n",
    "            df[c] = np.log1p(df[c])\n",
    "\n",
    "num_imp = SimpleImputer(strategy='median')\n",
    "X_num_train   = num_imp.fit_transform(train_c[num_feats])\n",
    "X_num_test    = num_imp.transform(test[num_feats])\n",
    "X_num_train_r = num_imp.transform(train_r[num_feats])\n",
    "\n",
    "# 3) Dates → engineered & imputed\n",
    "date_cols = ['CONSTRUCTION_START_DATE','SUBSTANTIAL_COMPLETION_DATE','invoiceDate']\n",
    "for df in (train_c, train_r, test):\n",
    "    for c in date_cols:\n",
    "        if c in df: df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "\n",
    "def date_feats(df):\n",
    "    out = {}\n",
    "    if 'CONSTRUCTION_START_DATE' in df and 'SUBSTANTIAL_COMPLETION_DATE' in df:\n",
    "        out['construction_duration_days'] = (df['SUBSTANTIAL_COMPLETION_DATE'] - df['CONSTRUCTION_START_DATE']).dt.days\n",
    "    if 'invoiceDate' in df:\n",
    "        out['invoice_year']       = df['invoiceDate'].dt.year\n",
    "        out['invoice_month']      = df['invoiceDate'].dt.month\n",
    "        out['invoice_day']        = df['invoiceDate'].dt.day\n",
    "        out['invoice_dayofweek']  = df['invoiceDate'].dt.dayofweek\n",
    "        out['invoice_quarter']    = df['invoiceDate'].dt.quarter\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "date_imp = SimpleImputer(strategy='median')\n",
    "X_date_train   = date_imp.fit_transform(date_feats(train_c))\n",
    "X_date_test    = date_imp.transform(date_feats(test))\n",
    "X_date_train_r = date_imp.transform(date_feats(train_r))\n",
    "\n",
    "# 4) Categorical (top-k one-hot)\n",
    "cat_cols = ['PROJECT_CITY','STATE','PROJECT_COUNTRY','CORE_MARKET','PROJECT_TYPE','UOM']\n",
    "\n",
    "def one_hot_topk(df, topk=20):\n",
    "    mats, maps = [], {}\n",
    "    for c in cat_cols:\n",
    "        if c in df:\n",
    "            top = df[c].value_counts().head(topk).index.tolist()\n",
    "            maps[c] = top\n",
    "            enc = df[c].fillna('missing').apply(lambda x: x if x in top else 'other')\n",
    "            cols = [ (enc == t).astype(np.int8).to_numpy() for t in (top + ['other']) ]\n",
    "            mats.append(np.column_stack(cols))\n",
    "    return (np.column_stack(mats) if mats else np.empty((len(df),0))), maps\n",
    "\n",
    "X_cat_train, cat_map = one_hot_topk(train_c)\n",
    "def apply_one_hot(df, cat_map):\n",
    "    mats = []\n",
    "    for c in cat_cols:\n",
    "        if c in df:\n",
    "            top = cat_map.get(c, [])\n",
    "            enc = df[c].fillna('missing').apply(lambda x: x if x in top else 'other')\n",
    "            mats.append(np.column_stack([(enc == t).astype(np.int8).to_numpy() for t in (top + ['other'])]))\n",
    "    return np.column_stack(mats) if mats else np.empty((len(df),0))\n",
    "\n",
    "X_cat_test    = apply_one_hot(test, cat_map)\n",
    "X_cat_train_r = apply_one_hot(train_r, cat_map)\n",
    "\n",
    "# 5) Stack (sparse)\n",
    "X_class_train = sparse.hstack([X_text_train,   sparse.csr_matrix(X_num_train),   sparse.csr_matrix(X_date_train),   sparse.csr_matrix(X_cat_train)]).tocsr()\n",
    "X_class_test  = sparse.hstack([X_text_test,    sparse.csr_matrix(X_num_test),    sparse.csr_matrix(X_date_test),    sparse.csr_matrix(X_cat_test)]).tocsr()\n",
    "X_reg_train   = sparse.hstack([X_text_train_r, sparse.csr_matrix(X_num_train_r), sparse.csr_matrix(X_date_train_r), sparse.csr_matrix(X_cat_train_r)]).tocsr()\n",
    "X_reg_test    = X_class_test.copy()\n",
    "\n",
    "y_class = train_c['MasterItemNo'].to_numpy()\n",
    "y_reg   = train_r['QtyShipped'].to_numpy()\n",
    "\n",
    "# Save with full suffix\n",
    "sparse.save_npz('X_class_train_full.npz', X_class_train)\n",
    "sparse.save_npz('X_class_test_full.npz',  X_class_test)\n",
    "sparse.save_npz('X_reg_train_full.npz',   X_reg_train)\n",
    "sparse.save_npz('X_reg_test_full.npz',    X_reg_test)\n",
    "np.save('y_class_full.npy', y_class)\n",
    "np.save('y_reg_full.npy',   y_reg)\n",
    "\n",
    "print(f\"Features ✓  (X_class_train {X_class_train.shape}, X_reg_train {X_reg_train.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85255b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost models on full dataset...\n",
      "No non-deterministic samples found. Using all samples for training.\n",
      "Original classes: 2572\n",
      "Classes after grouping rare ones: 504\n",
      "Classes after limiting to top 100: 101\n",
      "Training XGBoost Classifier...\n",
      "Classifier trained successfully!\n",
      "Training XGBoost Regressor...\n",
      "Regressor trained successfully!\n",
      "Models saved to disk.\n",
      "Making predictions...\n",
      "Submission file created: submission_xgb_full.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 3 — XGBoost Models (Full Dataset) - FIXED VERSION\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Training XGBoost models on full dataset...\")\n",
    "\n",
    "# Load full data\n",
    "train_c = pd.read_csv(\"clean_train_full.csv\")\n",
    "test_full = pd.read_csv(\"clean_test_full.csv\")\n",
    "Xc_tr = sparse.load_npz(\"X_class_train_full.npz\")\n",
    "Xc_te = sparse.load_npz(\"X_class_test_full.npz\")\n",
    "Xr_tr = sparse.load_npz(\"X_reg_train_full.npz\")\n",
    "Xr_te = sparse.load_npz(\"X_reg_test_full.npz\")\n",
    "yc = np.load(\"y_class_full.npy\")\n",
    "yr = np.load(\"y_reg_full.npy\")\n",
    "\n",
    "# Deterministic mapping for classification\n",
    "det = train_c.groupby(\"ItemDescription\")[\"MasterItemNo\"].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else None)\n",
    "det_items = det[det.notna()]\n",
    "\n",
    "# Prepare non-deterministic data for classification\n",
    "mask = ~train_c[\"ItemDescription\"].isin(det_items.index)\n",
    "Xc_nd, yc_nd = Xc_tr[mask.values], yc[mask.values]\n",
    "\n",
    "# Check if we have any non-deterministic samples\n",
    "if len(yc_nd) == 0:\n",
    "    print(\"No non-deterministic samples found. Using all samples for training.\")\n",
    "    Xc_nd, yc_nd = Xc_tr, yc\n",
    "\n",
    "# For classification, we'll use a simpler approach - reduce dimensionality\n",
    "print(f\"Original classes: {len(np.unique(yc_nd))}\")\n",
    "\n",
    "# Strategy 1: Group rare classes into \"other\" category\n",
    "class_counts = pd.Series(yc_nd).value_counts()\n",
    "rare_classes = class_counts[class_counts < 5].index\n",
    "yc_nd_processed = np.where(np.isin(yc_nd, rare_classes), 'other', yc_nd)\n",
    "\n",
    "# Count unique classes after grouping\n",
    "unique_classes = np.unique(yc_nd_processed)\n",
    "print(f\"Classes after grouping rare ones: {len(unique_classes)}\")\n",
    "\n",
    "# If still too many, use top N classes only\n",
    "if len(unique_classes) > 100:\n",
    "    top_classes = class_counts.head(100).index\n",
    "    yc_nd_processed = np.where(np.isin(yc_nd_processed, top_classes), yc_nd_processed, 'other')\n",
    "    unique_classes = np.unique(yc_nd_processed)\n",
    "    print(f\"Classes after limiting to top 100: {len(unique_classes)}\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "yc_enc = le.fit_transform(yc_nd_processed)\n",
    "\n",
    "# Only split if we have enough samples\n",
    "if len(yc_enc) > 1:\n",
    "    Xc_train, Xc_val, yc_train, yc_val = train_test_split(\n",
    "        Xc_nd, yc_enc, test_size=0.2, random_state=SEED, stratify=yc_enc\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough samples for validation split, using all for training\")\n",
    "    Xc_train, Xc_val, yc_train, yc_val = Xc_nd, Xc_nd[:0], yc_enc, yc_enc[:0]\n",
    "\n",
    "# Convert to dense for XGBoost\n",
    "if hasattr(Xc_train, 'toarray'):\n",
    "    Xc_train = Xc_train.toarray()\n",
    "if hasattr(Xc_val, 'toarray') and Xc_val.shape[0] > 0:\n",
    "    Xc_val = Xc_val.toarray()\n",
    "\n",
    "# XGBoost Classifier - only train if we have samples\n",
    "if len(yc_enc) > 0:\n",
    "    print(\"Training XGBoost Classifier...\")\n",
    "    \n",
    "    # Use simpler parameters for multi-class\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=SEED,\n",
    "        tree_method='hist',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        xgb_classifier.fit(Xc_train, yc_train)\n",
    "        print(\"Classifier trained successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training classifier: {e}\")\n",
    "        # Fallback: use LightGBM if available, or skip classification\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            print(\"Trying LightGBM as fallback...\")\n",
    "            xgb_classifier = lgb.LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            xgb_classifier.fit(Xc_train, yc_train)\n",
    "            print(\"LightGBM classifier trained successfully!\")\n",
    "        except:\n",
    "            print(\"Both XGBoost and LightGBM failed. Will use deterministic mapping only.\")\n",
    "            xgb_classifier = None\n",
    "else:\n",
    "    print(\"No samples for classification training\")\n",
    "    xgb_classifier = None\n",
    "\n",
    "# Split regression data\n",
    "Xr_train, Xr_val, yr_train, yr_val = train_test_split(\n",
    "    Xr_tr, yr, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Convert to dense for XGBoost\n",
    "if hasattr(Xr_train, 'toarray'):\n",
    "    Xr_train = Xr_train.toarray()\n",
    "if hasattr(Xr_val, 'toarray'):\n",
    "    Xr_val = Xr_val.toarray()\n",
    "\n",
    "# XGBoost Regressor\n",
    "print(\"Training XGBoost Regressor...\")\n",
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=SEED,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "try:\n",
    "    xgb_regressor.fit(Xr_train, yr_train)\n",
    "    print(\"Regressor trained successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training regressor: {e}\")\n",
    "    # Fallback: use simpler parameters\n",
    "    xgb_regressor = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    xgb_regressor.fit(Xr_train, yr_train)\n",
    "    print(\"Regressor trained with fallback parameters!\")\n",
    "\n",
    "# Save models for later use\n",
    "import joblib\n",
    "if xgb_classifier is not None:\n",
    "    joblib.dump(xgb_classifier, 'xgb_classifier_full.pkl')\n",
    "    # Also save the mapping from processed classes back to original\n",
    "    class_mapping = {}\n",
    "    for processed_label, original_label in zip(yc_nd_processed, yc_nd):\n",
    "        if processed_label not in class_mapping:\n",
    "            class_mapping[processed_label] = original_label\n",
    "    joblib.dump(class_mapping, 'class_mapping_full.pkl')\n",
    "    \n",
    "joblib.dump(xgb_regressor, 'xgb_regressor_full.pkl')\n",
    "joblib.dump(le, 'label_encoder_full.pkl')\n",
    "joblib.dump(det_items, 'deterministic_mapping_full.pkl')\n",
    "\n",
    "print(\"Models saved to disk.\")\n",
    "\n",
    "# ===============================\n",
    "# Cell 4 — Prediction & Submission - FIXED VERSION\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "# Load test data and models\n",
    "test_full = pd.read_csv(\"clean_test_full.csv\")\n",
    "Xc_te = sparse.load_npz(\"X_class_test_full.npz\")\n",
    "Xr_te = sparse.load_npz(\"X_reg_test_full.npz\")\n",
    "\n",
    "# Load saved models and mappings\n",
    "try:\n",
    "    xgb_classifier = joblib.load('xgb_classifier_full.pkl')\n",
    "    classifier_available = True\n",
    "    class_mapping = joblib.load('class_mapping_full.pkl')\n",
    "except:\n",
    "    print(\"Classifier not available, using deterministic mapping only\")\n",
    "    xgb_classifier = None\n",
    "    classifier_available = False\n",
    "\n",
    "xgb_regressor = joblib.load('xgb_regressor_full.pkl')\n",
    "le = joblib.load('label_encoder_full.pkl')\n",
    "det_items = joblib.load('deterministic_mapping_full.pkl')\n",
    "\n",
    "# Convert test data to dense\n",
    "if hasattr(Xc_te, 'toarray'):\n",
    "    Xc_te_dense = Xc_te.toarray()\n",
    "else:\n",
    "    Xc_te_dense = Xc_te\n",
    "\n",
    "if hasattr(Xr_te, 'toarray'):\n",
    "    Xr_te_dense = Xr_te.toarray()\n",
    "else:\n",
    "    Xr_te_dense = Xr_te\n",
    "\n",
    "# Classification predictions\n",
    "pred_master = np.empty(test_full.shape[0], dtype=object)\n",
    "is_det = test_full[\"ItemDescription\"].isin(det_items.index).values\n",
    "\n",
    "# Apply deterministic mapping where possible\n",
    "pred_master[is_det] = test_full.loc[is_det, \"ItemDescription\"].map(det_items).to_numpy()\n",
    "\n",
    "# Predict non-deterministic items using XGBoost if available\n",
    "if (~is_det).any() and classifier_available:\n",
    "    Xc_test_nd = Xc_te_dense[~is_det]\n",
    "    try:\n",
    "        pred_encoded = xgb_classifier.predict(Xc_test_nd)\n",
    "        pred_processed = le.inverse_transform(pred_encoded)\n",
    "        \n",
    "        # Map processed labels back to original MasterItemNo\n",
    "        pred_original = np.array([class_mapping.get(label, 'other') for label in pred_processed])\n",
    "        pred_master[~is_det] = pred_original\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification prediction: {e}\")\n",
    "        # Fallback: use most common MasterItemNo\n",
    "        most_common_class = train_c['MasterItemNo'].mode()[0]\n",
    "        pred_master[~is_det] = most_common_class\n",
    "elif (~is_det).any():\n",
    "    # If no classifier available, use most common for all non-deterministic items\n",
    "    most_common_class = train_c['MasterItemNo'].mode()[0]\n",
    "    pred_master[~is_det] = most_common_class\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "if np.any(pd.isna(pred_master)):\n",
    "    most_common_master = train_c['MasterItemNo'].mode()[0]\n",
    "    pred_master[pd.isna(pred_master)] = most_common_master\n",
    "\n",
    "# Ensure proper data types (keep as string)\n",
    "pred_master = pred_master.astype(str)\n",
    "\n",
    "# Regression predictions\n",
    "try:\n",
    "    pred_qty = xgb_regressor.predict(Xr_te_dense)\n",
    "    pred_qty = np.clip(pred_qty, 1, None).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error in regression prediction: {e}\")\n",
    "    # Fallback: use median quantity\n",
    "    median_qty = int(np.median(yr))\n",
    "    pred_qty = np.full(Xr_te_dense.shape[0], median_qty, dtype=int)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_full[\"id\"],\n",
    "    \"MasterItemNo\": pred_master,\n",
    "    \"QtyShipped\": pred_qty\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_xgb_full.csv\", index=False)\n",
    "print(\"Submission file created: submission_xgb_full.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6537e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models...\n",
      "Error in regression evaluation: name 'Xr_val_dense' is not defined\n",
      "Error in classification evaluation: name 'is_ovr' is not defined\n",
      "📊 XGBoost Evaluation Results\n",
      "Accuracy       : 0.5000\n",
      "F1 Score       : 0.5000\n",
      "MAE            : 205.1760\n",
      "RegressionScore: 0.9553\n",
      "Final Score    : 0.7276\n",
      "\n",
      "📊 Dataset Info:\n",
      "Classification training samples: 11228\n",
      "Classification validation samples: 2808\n",
      "Regression training samples: 11200\n",
      "Regression validation samples: 2801\n",
      "Unique classes: 101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# Cell 5 — Evaluation - FIXED\n",
    "# ===============================\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "\n",
    "print(\"Evaluating models...\")\n",
    "\n",
    "# Regression evaluation\n",
    "try:\n",
    "    yr_pred = xgb_regressor.predict(Xr_val_dense)\n",
    "    mae = mean_absolute_error(yr_val, yr_pred)\n",
    "    print(f\"Regression - MAE: {mae:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in regression evaluation: {e}\")\n",
    "    mae = np.mean(yr_val)  # Default MAE if evaluation fails\n",
    "\n",
    "# Classification evaluation (only if we have validation samples)\n",
    "if Xc_val.shape[0] > 0 and classifier_available:\n",
    "    try:\n",
    "        if is_ovr:\n",
    "            # For OneVsRest classifier\n",
    "            pred_probs = xgb_classifier.predict_proba(Xc_val_dense)\n",
    "            yc_pred_encoded = np.argmax(pred_probs, axis=1)\n",
    "        else:\n",
    "            # For regular XGBoost classifier\n",
    "            yc_pred_encoded = xgb_classifier.predict(Xc_val_dense)\n",
    "        \n",
    "        yc_pred = le.inverse_transform(yc_pred_encoded)\n",
    "        yc_val_original = le.inverse_transform(yc_val)\n",
    "        \n",
    "        accuracy = accuracy_score(yc_val_original, yc_pred)\n",
    "        f1 = f1_score(yc_val_original, yc_pred, average=\"weighted\")\n",
    "        print(f\"Classification - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification evaluation: {e}\")\n",
    "        accuracy, f1 = 0.5, 0.5  # Default scores if evaluation fails\n",
    "else:\n",
    "    print(\"No validation samples for classification evaluation\")\n",
    "    accuracy, f1 = 1.0, 1.0  # Assume perfect if no validation data\n",
    "\n",
    "# Calculate normalized regression score\n",
    "if yr_val.max() == yr_val.min():\n",
    "    reg_score = 1.0\n",
    "else:\n",
    "    norm_mae = mae / (yr_val.max() - yr_val.min())\n",
    "    reg_score = 1 - max(0, min(norm_mae, 1))\n",
    "\n",
    "# Final score calculation\n",
    "final_score = 0.25 * accuracy + 0.25 * f1 + 0.5 * reg_score\n",
    "\n",
    "print(\"📊 XGBoost Evaluation Results\")\n",
    "print(f\"Accuracy       : {accuracy:.4f}\")\n",
    "print(f\"F1 Score       : {f1:.4f}\")\n",
    "print(f\"MAE            : {mae:.4f}\")\n",
    "print(f\"RegressionScore: {reg_score:.4f}\")\n",
    "print(f\"Final Score    : {final_score:.4f}\")\n",
    "\n",
    "# Show basic info\n",
    "print(f\"\\n📊 Dataset Info:\")\n",
    "print(f\"Classification training samples: {len(yc_train)}\")\n",
    "print(f\"Classification validation samples: {len(yc_val)}\")\n",
    "print(f\"Regression training samples: {len(yr_train)}\")\n",
    "print(f\"Regression validation samples: {len(yr_val)}\")\n",
    "print(f\"Unique classes: {len(np.unique(yc_enc))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
